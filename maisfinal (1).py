# -*- coding: utf-8 -*-
"""MaisFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kWCLwqgCV3ekrgNt3k-WyIJ97TC7A03q
"""

#import libraries

import pandas as pd
import numpy as np
import re
from collections import Counter

#download data set and sort into lists of text and language information
df = pd.read_csv("https://raw.githubusercontent.com/fiaosborne/MaisFinal/main/Language_Detection.csv")

text = df["Text"].tolist()
for i in (range(len(text))):
  text[i] = str(text[i])
language = df["Language"].tolist()

#function to clean text of upper case letters and unwanted characters
def clean(txt):
  #txt = txt.lower()
  txt = txt.lower()
  txt = re.sub('<.*?>','',txt)
  txt = re.sub('\[.*?\]','',txt)
  txt = re.sub('\(.*?\)','',txt)
  txt = re.sub(r'[0-9]','',txt)
  txt = re.sub(r'[!.,\'\"-]','', txt)
  return txt

#clean all text data
txt_data = []
for line in text:
  txt_data.append(clean(line))

# current language information is inputed as a string, write a function to translate string into int
def lang_to_int(lang):
    if lang == "English":
      return 1
    elif lang == "Malayalam":
      return 2
    elif lang == "Hindi":
      return 3
    elif lang == "Tamil":
      return 4
    elif lang ==  "Kannada":
      return 5
    elif lang ==  "French":
      return 6
    elif lang ==  "Spanish":
      return 7
    elif lang ==  "Portugeese":
      return 8
    elif lang ==  "Italian":
      return 9
    elif lang ==  "Russian":
      return 10
    elif lang ==  "Sweedish":
      return 11
    elif lang ==  "Dutch":
      return 12
    elif lang ==  "Arabic":
      return 13
    elif lang ==  "Turkish":
      return 14
    elif lang ==  "German":
      return 15
    elif lang ==  "Danish":
      return 16
    elif lang ==  "Greek":
      return 17

#translate language data into ints
for i in (range(len(language))):
  language[i] = lang_to_int(language[i])

#count the number of data points of each language present in our data
count = [0]*17
for i in language:
  count[i-1]+=1

print(count)

#split data into test, validate, and train subsets
txt_test = []
lang_test = []
txt_train = []
lang_train = []
txt_val = []
lang_val=[]
i = 0
while i < len(txt_data):
  if i%10 == 0:
    txt_val.append(txt_data[i])
    lang_val.append(language[i])
  elif i % 5 == 0:
    txt_test.append(txt_data[i])
    lang_test.append(language[i])
  else:
    txt_train.append(txt_data[i])
    lang_train.append(language[i])
  
  i+=1

#function to generate list of vocabulary from input text
def get_vocab(text_list,vocab_size):

  text = [j for i in text_list for j in i.split()]

  r = Counter(text)
  vocab = {}
  common = r.most_common(vocab_size)
  for i in range(len(common)):
    vocab[common[i][0]]=i


  return vocab

#generate list of vocabulary
vocab_size = 10000
vocab = get_vocab(txt_test,vocab_size)

#function to turn all text data points into vectors
def vectorize(txt_str,vocab):

  txt = txt_str.split()
  txt_dict = dict.fromkeys(txt,0)

  t = [0]*len(vocab)
  for key in txt_dict.keys():
    if key in vocab:
      index = vocab[key]
      t[index] = 1
  return t

#turn data from all subsets into vectors
txt_train_vect = []

for i in (range(len(txt_train))):
  txt_train_vect.append(vectorize(txt_train[i],vocab))

txt_test_vect = []

for i in (range(len(txt_test))):
  txt_test_vect.append(vectorize(txt_test[i],vocab))

txt_val_vect = []

for i in (range(len(txt_val))):
  txt_val_vect.append(vectorize(txt_val[i],vocab))

# get probability that text belongs to particular language should it contain certain word in vocabulary
Pr_txt_en = [0]*len(vocab)
Pr_txt_ma = [0]*len(vocab)
Pr_txt_hi = [0]*len(vocab)
Pr_txt_ta = [0]*len(vocab)
Pr_txt_ka = [0]*len(vocab)
Pr_txt_fr = [0]*len(vocab)
Pr_txt_sp = [0]*len(vocab)
Pr_txt_po = [0]*len(vocab)
Pr_txt_it = [0]*len(vocab)
Pr_txt_ru = [0]*len(vocab)
Pr_txt_sw = [0]*len(vocab)
Pr_txt_du = [0]*len(vocab)
Pr_txt_ar = [0]*len(vocab)
Pr_txt_tu = [0]*len(vocab)
Pr_txt_ge = [0]*len(vocab)
Pr_txt_da = [0]*len(vocab)
Pr_txt_gr = [0]*len(vocab)

for i in (range(len(vocab))):
  for j in range(len(lang_train)):
    if lang_train[j] == 1:
      if txt_train_vect[j][i] == 1:
        Pr_txt_en[i] += 1
    elif lang_train[j] == 2:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ma[i] += 1
    elif lang_train[j] == 3:
      if txt_train_vect[j][i] == 1:
        Pr_txt_hi[i] += 1
    elif lang_train[j] == 4:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ta[i] += 1
    elif lang_train[j] == 5:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ka[i] += 1
    elif lang_train[j] == 6:
      if txt_train_vect[j][i] == 1:
        Pr_txt_fr[i] += 1
    elif lang_train[j] == 7:
      if txt_train_vect[j][i] == 1:
        Pr_txt_sp[i] += 1
    elif lang_train[j] == 8:
      if txt_train_vect[j][i] == 1:
        Pr_txt_po[i] += 1
    elif lang_train[j] == 9:
      if txt_train_vect[j][i] == 1:
        Pr_txt_it[i] += 1
    elif lang_train[j] == 10:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ru[i] += 1
    elif lang_train[j] == 11:
      if txt_train_vect[j][i] == 1:
        Pr_txt_sw[i] += 1
    elif lang_train[j] == 12:
      if txt_train_vect[j][i] == 1:
        Pr_txt_du[i] += 1
    elif lang_train[j] == 13:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ar[i] += 1
    elif lang_train[j] == 14:
      if txt_train_vect[j][i] == 1:
        Pr_txt_tu[i] += 1
    elif lang_train[j] == 15:
      if txt_train_vect[j][i] == 1:
        Pr_txt_ge[i] += 1
    elif lang_train[j] == 16:
      if txt_train_vect[j][i] == 1:
        Pr_txt_da[i] += 1
    elif lang_train[j] == 17:
      if txt_train_vect[j][i] == 1:
        Pr_txt_gr[i] += 1

Pr_lang = [0]*17

for i in (range(len(Pr_lang))):
  Pr_lang[i] = lang_train.count(i+1)/len(lang_train)

Pr_x = [[0]*len(vocab)]*17
for i in (range(len(vocab))):
  Pr_txt_en[i] =  Pr_txt_en[i]/lang_train.count(1)
  Pr_txt_ma[i] =  Pr_txt_ma[i]/lang_train.count(2)
  Pr_txt_hi[i] =  Pr_txt_hi[i]/lang_train.count(3)
  Pr_txt_ta[i] =  Pr_txt_ta[i]/lang_train.count(4)
  Pr_txt_ka[i] =  Pr_txt_ka[i]/lang_train.count(5)
  Pr_txt_fr[i] =  Pr_txt_fr[i]/lang_train.count(6)
  Pr_txt_sp[i] =  Pr_txt_sp[i]/lang_train.count(7)
  Pr_txt_po[i] =  Pr_txt_po[i]/lang_train.count(8)
  Pr_txt_it[i] =  Pr_txt_it[i]/lang_train.count(9)
  Pr_txt_ru[i] =  Pr_txt_ru[i]/lang_train.count(10)
  Pr_txt_sw[i] =  Pr_txt_sw[i]/lang_train.count(11)
  Pr_txt_du[i] =  Pr_txt_du[i]/lang_train.count(12)
  Pr_txt_ar[i] =  Pr_txt_ar[i]/lang_train.count(13)
  Pr_txt_tu[i] =  Pr_txt_tu[i]/lang_train.count(14)
  Pr_txt_ge[i] =  Pr_txt_ge[i]/lang_train.count(15)
  Pr_txt_da[i] =  Pr_txt_da[i]/lang_train.count(16)
  Pr_txt_gr[i] =  Pr_txt_gr[i]/lang_train.count(17)

def to_lang_str(int):
    if int == 1:
      return "English"
    elif int == 2:
      return "Malayalam"
    elif int == 3:
      return "Hindi"
    elif int == 4:
      return "Tamil"
    elif int == 5:
      return "Kannada"
    elif int ==  6:
      return "French"
    elif int == 7:
      return "Spanish"
    elif int == 8:
      return "Portugese"
    elif int ==  9:
      return "Italian"
    elif int ==  10:
      return "Russian"
    elif int ==  11:
      return "Swedish"
    elif int ==  12:
      return "Dutch"
    elif int ==  13:
      return "Arabic"
    elif int ==  14:
      return "Turkish"
    elif int ==  15:
      return "German"
    elif int ==  16:
      return "Danish"
    elif int == 17:
      return "Greek"

def naive_bayess(vec):

  prob_en = Pr_lang[0]
  prob_ma = Pr_lang[1]
  prob_hi = Pr_lang[2]
  prob_ta = Pr_lang[3]
  prob_ka = Pr_lang[4]
  prob_fr = Pr_lang[5]
  prob_sp = Pr_lang[6]
  prob_po = Pr_lang[7]
  prob_it = Pr_lang[8]
  prob_ru = Pr_lang[9]
  prob_sw = Pr_lang[10]
  prob_du = Pr_lang[11]
  prob_ar = Pr_lang[12]
  prob_tu = Pr_lang[13]
  prob_ge = Pr_lang[14]
  prob_da = Pr_lang[15]
  prob_gr = Pr_lang[16]

  for i in (range(len(vec))):
    if vec[i] == 1:
      prob_en = prob_en*Pr_txt_en[i]
      prob_ma = prob_ma*Pr_txt_ma[i]
      prob_hi = prob_hi*Pr_txt_hi[i]
      prob_ta = prob_ta*Pr_txt_ta[i]
      prob_ka = prob_ka*Pr_txt_ka[i]
      prob_fr = prob_fr*Pr_txt_fr[i]
      prob_sp = prob_sp*Pr_txt_sp[i]
      prob_po = prob_po*Pr_txt_po[i]
      prob_it = prob_it*Pr_txt_it[i]
      prob_ru = prob_ru*Pr_txt_ru[i]
      prob_sw = prob_sw*Pr_txt_sw[i]
      prob_du = prob_du*Pr_txt_du[i]
      prob_ar = prob_ar*Pr_txt_ar[i]
      prob_tu = prob_tu*Pr_txt_tu[i]
      prob_ge = prob_ge*Pr_txt_ge[i]
      prob_da = prob_da*Pr_txt_da[i]
      prob_gr = prob_gr*Pr_txt_gr[i]

  
  probability = [prob_en, prob_ma, prob_hi, prob_ta, prob_ka, prob_fr, prob_sp, prob_po, prob_it, prob_ru, prob_sw, prob_du, prob_ar, prob_tu, prob_ge, prob_da, prob_gr]
  
  max = probability[0]
  ind = 0
  for i in (range(len(probability))):
    if probability[i] > max:
      max = probability[i]
      ind = i
  
  lang = to_lang_str(ind+1)

  return lang

from sklearn.metrics import accuracy_score
predictor = []

for i in (range(len(txt_val_vect))):
  predictor.append(naive_bayess(txt_val_vect[i]))

print(lang_val)
print(predictor)

accuracy = accuracy_score(lang_val,predictor,normalize=True,sample_weight=None)
print(accuracy)

